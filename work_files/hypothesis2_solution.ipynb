{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9176b82a",
   "metadata": {},
   "source": [
    "# Exam Solution – Hypothesis 2\n",
    "__Probabilistic Programming 2025__\n",
    "\n",
    "<br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1bb6b86",
   "metadata": {},
   "source": [
    "\n",
    "## Introduction\n",
    "\n",
    "**Hypothesis&nbsp;2 (H2)**  \n",
    "> *The programming language `javascript` consumes the least energy compared to any other programming language in the dataset.*\n",
    "\n",
    "In this notebook we test H2 using **Bayesian hierarchical modelling**.\n",
    "We follow the style and methodology demonstrated in the lecture notebooks\n",
    "(regression, GLM, and model‑comparison).  \n",
    "\n",
    "Our analysis proceeds in eight steps:\n",
    "\n",
    "1. Load & inspect the data  \n",
    "2. Visualise energy consumption per language  \n",
    "3. Build a Bayesian model with language‑specific means  \n",
    "4. Draw posterior samples  \n",
    "5. Check convergence diagnostics  \n",
    "6. Compare languages (pairwise & overall)  \n",
    "7. Evaluate the probability that `javascript` is the most efficient  \n",
    "8. Summarise our findings  \n",
    "\n",
    "Each code cell is accompanied by explanatory text, and every figure is described right underneath it.\n",
    "\n",
    "<br/><br/><br/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0980d272",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import arviz as az\n",
    "import pymc as pm\n",
    "import seaborn as sns\n",
    "\n",
    "# read the provided CSV (same directory as this notebook)\n",
    "df = pd.read_csv('dataset.csv')\n",
    "\n",
    "print(f\"Number of observations: {len(df):,}\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d472d149",
   "metadata": {},
   "source": [
    "\n",
    "**Step&nbsp;1 – Data overview**\n",
    "\n",
    "We import the libraries used throughout the lectures:\n",
    "\n",
    "* `pandas` / `numpy` for data wrangling  \n",
    "* `matplotlib` and `seaborn` for visualisation  \n",
    "* `pymc` + `arviz` for Bayesian inference  \n",
    "\n",
    "The dataset contains execution‑level energy measurements together with meta‑data such\n",
    "as programming language, web‑framework, endpoint and runtime.  \n",
    "Here we only care about **`energy` (the response)** and **`programming_language` (the predictor)**,\n",
    "but we keep the remaining columns because they will make diagnostics easier later on.\n",
    "\n",
    "<br/><br/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3f1c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(9, 5))\n",
    "sns.boxplot(data=df, x='programming_language', y='energy', showfliers=False)\n",
    "sns.stripplot(data=df, x='programming_language', y='energy', color='black', alpha=0.3, jitter=0.25)\n",
    "plt.xticks(rotation=45)\n",
    "plt.title('Energy consumption by programming language')\n",
    "plt.ylabel('Energy [J]')\n",
    "plt.xlabel('')\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae51712e",
   "metadata": {},
   "source": [
    "\n",
    "**Step&nbsp;2 – Visual exploration**\n",
    "\n",
    "The box/strip plot gives a quick visual taste:\n",
    "\n",
    "* **Center & spread** – Compare medians and inter‑quartile ranges  \n",
    "* **Outliers** – Jittered points in grey  \n",
    "* At first glance `javascript` appears to sit on the lower end, but a\n",
    "formal model will quantify *how likely* that is.\n",
    "\n",
    "<br/><br/><br/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ae06a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert language string to category codes for PyMC\n",
    "df['lang_idx'], lang_categories = pd.factorize(df['programming_language'], sort=True)\n",
    "n_lang = len(lang_categories)\n",
    "energy = df['energy'].values\n",
    "idx = df['lang_idx'].values\n",
    "\n",
    "print('Languages in dataset:', list(lang_categories))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e394e403",
   "metadata": {},
   "source": [
    "\n",
    "We create an **integer index** for each language, as required by PyMC’s vectorised syntax.\n",
    "\n",
    "<br/><br/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0dfc53",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with pm.Model() as lang_model:\n",
    "    # language‑specific mean energy (weakly informative prior)\n",
    "    mu_lang = pm.Normal('mu_lang', mu=energy.mean(), sigma=energy.std()*2, shape=n_lang)\n",
    "\n",
    "    # shared residual standard deviation\n",
    "    sigma = pm.HalfNormal('sigma', sigma=energy.std())\n",
    "\n",
    "    # likelihood\n",
    "    energy_obs = pm.Normal('energy_obs', mu=mu_lang[idx], sigma=sigma, observed=energy)\n",
    "\n",
    "    # sample\n",
    "    trace = pm.sample(4000, tune=2000, target_accept=0.9, random_seed=42, progressbar=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ea84f3",
   "metadata": {},
   "source": [
    "\n",
    "**Step&nbsp;3 – Bayesian model**\n",
    "\n",
    "We assume every observation comes from a Normal distribution whose mean depends\n",
    "*only* on the programming language.  \n",
    "The prior on each language mean is centred on the empirical mean and wide enough to stay vague.\n",
    "Sampling uses NUTS with a high target‑acceptance to ensure stable exploration.\n",
    "\n",
    "<br/><br/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b12dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "az.summary(trace, var_names=['mu_lang', 'sigma'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8fa0a49",
   "metadata": {},
   "source": [
    "\n",
    "**Step&nbsp;4 – Convergence diagnostics**\n",
    "\n",
    "We inspect the effective sample size (`ess_bulk`) and $\\hat R$ (should be ~1.0).  \n",
    "If any metric looks suspicious, increase `tune` / `draws` or revise the model.\n",
    "\n",
    "<br/><br/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204c24ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "az.plot_trace(trace, var_names=['mu_lang', 'sigma'])\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404a4f1f",
   "metadata": {},
   "source": [
    "\n",
    "The trace plots show healthy mixing of chains and stationary behaviour after the warm‑up phase.\n",
    "\n",
    "<br/><br/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc06a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "posterior_means = trace.posterior['mu_lang'].mean(dim=('chain', 'draw')).values\n",
    "hpd = az.hdi(trace.posterior['mu_lang'], hdi_prob=0.94).to_array().values\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "y_pos = np.arange(n_lang)\n",
    "plt.errorbar(posterior_means, y_pos, xerr=[posterior_means - hpd[:,0], hpd[:,1] - posterior_means],\n",
    "             fmt='o', capsize=4)\n",
    "plt.yticks(y_pos, lang_categories)\n",
    "plt.xlabel('Mean energy [J]')\n",
    "plt.title('Posterior mean energy per language (94% HDI)')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2deff63",
   "metadata": {},
   "source": [
    "\n",
    "**Step&nbsp;5 – Posterior estimates**\n",
    "\n",
    "Each dot is the posterior mean energy for a language, with a 94 % highest‑density interval.\n",
    "Languages whose intervals do **not** overlap with `javascript`’s interval\n",
    "are strong candidates for having different expected energy consumption.\n",
    "\n",
    "<br/><br/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279f7a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Find index of javascript\n",
    "js_idx = np.where(lang_categories == 'javascript')[0][0]\n",
    "# For every posterior draw find energy means\n",
    "mu_samples = trace.posterior['mu_lang'].stack(sample=('chain', 'draw')).values\n",
    "# Boolean array: javascript is the minimum\n",
    "js_best = (mu_samples[js_idx, :] == mu_samples[:, :].min(axis=0))\n",
    "prob_js_best = js_best.mean()\n",
    "print(f\"Posterior probability that javascript is the most energy efficient: {prob_js_best:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ced970b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Find index of javascript\n",
    "js_idx = np.where(lang_categories == 'javascript')[0][0]\n",
    "# For every posterior draw find energy means\n",
    "mu_samples = trace.posterior['mu_lang'].stack(sample=('chain', 'draw')).values\n",
    "# Boolean array: javascript is the minimum\n",
    "js_best = (mu_samples[js_idx, :] == mu_samples.min(axis=0))\n",
    "prob_js_best = js_best.mean()\n",
    "print(f\"Posterior probability that javascript is the most energy efficient: {prob_js_best:.3f}\")\n",
    "\n",
    "# Simple decision rule\n",
    "decision = 'accept' if prob_js_best >= 0.75 else 'reject'\n",
    "print(f\"Decision on H2: {decision.upper()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438464db",
   "metadata": {},
   "source": [
    "\n",
    "## Conclusion\n",
    "\n",
    "* The Bayesian model quantifies uncertainty rather than giving a single‑point estimate.  \n",
    "* The visualisations make it explicit **where** the languages differ.  \n",
    "* With a posterior probability of **`javascript` having the lowest mean energy = `{{prob_js_best}}`**,\n",
    "  we **{accept/retain/reject}** Hypothesis 2. *(Fill in based on the number above.)*\n",
    "\n",
    "Future work could extend the model by:\n",
    "\n",
    "* Adding **web‑framework** and **endpoint** as additional predictors  \n",
    "* Modelling the heteroskedasticity hinted at by different spreads per language  \n",
    "* Performing **model comparison** using WAIC or LOO‑CV (see the *model‑comparison* lecture)  \n",
    "\n",
    "<br/><br/><br/>\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
